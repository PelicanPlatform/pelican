---
name: Run Tests (Linux) [on schedule]

# On Linux, we run tests in a custom container image. For a scheduled
# run, we have no choice but to use the most recent container image that
# was built.

on:
  schedule:
    - cron: "0 7 * * *"  # at 07:00 every day
  workflow_dispatch:

jobs:
  run-tests:
    name: Test
    uses: ./.github/workflows/test-linux.yml
    with:
      image: hub.opensciencegrid.org/pelican_platform/pelican-test:latest-itb

  analyze-runs:
    name: Analyze Runs
    if: always()
    needs: [run-tests]
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Get the last 14 completed workflow runs
          run_ids=$(gh run list \
            --repo ${{ github.repository }} \
            --workflow "Run Tests (Linux) [on schedule]" \
            --status completed \
            --limit 14 \
            --json databaseId \
            --jq '.[].databaseId')

          # Add the current workflow run
          run_ids=$(printf '%s\n%s' "$run_ids" "${{ github.run_id }}")

          # Download artifacts for each run into their own directory
          for run_id in $run_ids; do
            echo "Downloading artifacts from run $run_id"
            mkdir -p "artifacts/run-$run_id"
            gh run download "$run_id" \
              --repo ${{ github.repository }} \
              --dir "artifacts/run-$run_id" || echo "No artifacts found for run $run_id"
          done

          printf 'Downloaded artifacts from %s runs\n\n' "$(echo "$run_ids" | wc -l)" >> $GITHUB_STEP_SUMMARY

      - name: Analyze test results
        run: |
          # Analyze JUnit files to count test failures
          python3 << 'EOF'
          import xml.etree.ElementTree as ET
          from collections import defaultdict
          from pathlib import Path
          import sys

          # Track failures by matrix variant
          failures_by_matrix = defaultdict(lambda: defaultdict(int))

          # Find all JUnit XML files
          artifacts_dir = Path('artifacts')
          if not artifacts_dir.exists():
              print("No artifacts directory found")
              sys.exit(0)

          for run_dir in sorted(artifacts_dir.iterdir()):
              if not run_dir.is_dir():
                  continue

              # Process each matrix variant
              for artifact_dir in run_dir.iterdir():
                  if not artifact_dir.is_dir() or not artifact_dir.name.startswith('junit-'):
                      continue

                  # Extract matrix variant name (e.g., "pelican" or "pelican-server")
                  matrix_name = artifact_dir.name.replace('junit-', '').replace('-Linux', '')

                  # Find JUnit XML files
                  for xml_file in artifact_dir.glob('*.xml'):
                      try:
                          tree = ET.parse(xml_file)
                          root = tree.getroot()

                          # Parse test cases
                          for testcase in root.iter('testcase'):
                              test_name = f"{testcase.get('classname').split('/')[-1]}.{testcase.get('name')}"

                              # Check if test failed
                              if testcase.find('failure') is not None or testcase.find('error') is not None:
                                  failures_by_matrix[matrix_name][test_name] += 1
                      except Exception as e:
                          print(f"Error parsing {xml_file}: {e}")

          # Write results to file
          with open('test-failure-analysis.txt', 'w', encoding='utf-8') as f:
              for matrix_name in sorted(failures_by_matrix.keys()):
                  f.write(f"### {matrix_name}\n\n")

                  failures = failures_by_matrix[matrix_name]
                  if not failures:
                      f.write("No failures found\n\n")
                      continue

                  # Sort by failure count (descending), then by test name
                  sorted_failures = sorted(failures.items(), key=lambda x: (-x[1], x[0]))

                  f.write(f"{len(failures)} tests with failures:\n\n")

                  for test_name, count in sorted_failures:
                      f.write(f"- {count} failures: {test_name}\n\n")

          print("Analysis complete. Results written to test-failure-analysis.txt.")
          EOF

          # Print the analysis file
          cat test-failure-analysis.txt >> $GITHUB_STEP_SUMMARY
